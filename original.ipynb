{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "x4xotR5ai-0m",
        "JcP-QF8BDpDC",
        "xo9Lg1DMl4pk",
        "iKUFD3nwEbz5",
        "YoTpHYaZE5OX"
      ],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "DO NOT CHANGE EXCEPT ARGS\n",
        "\n",
        "\n",
        "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes\n",
        "\n",
        "Shivam Garg*, Dimitris Tsipras*, Percy Liang, Gregory Valiant\n",
        "\n",
        "Paper: http://arxiv.org/abs/2208.01066\n",
        "\n",
        "\n",
        "https://github.com/dtsip/in-context-learning.git\n"
      ],
      "metadata": {
        "id": "_d5EUe2h4jGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prerequisites\n",
        "\n",
        "tested on Python 3.10 with a T4 GPU"
      ],
      "metadata": {
        "id": "XgwnF6TW40UT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -q matplotlib==3.5.2 numpy==1.22.3 pandas==1.4.2 scikit-learn==1.0.2 seaborn==0.11.2 tqdm==4.64.0 transformers==4.17.0 wandb==0.12.11 xgboost==1.6.1 protobuf==3.20.1 torch==2.1.0+cu118"
      ],
      "metadata": {
        "id": "prnYu7oW6Usg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Config\n"
      ],
      "metadata": {
        "id": "jOAqiGNaqBrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class args_c():\n",
        "    class wandb_c():\n",
        "        def __init__(self):             # wandb\n",
        "            self.project = \"in-context-learning\"\n",
        "            self.name = \"linear_regression_toy\"\n",
        "            self.entity = \"your-entity\"\n",
        "            self.notes = \"\"\n",
        "            self.log_every_steps = 100\n",
        "\n",
        "    class model_c():\n",
        "        def __init__(self):             # model\n",
        "            self.family = \"gpt2\"\n",
        "            self.n_embd = 256\n",
        "            self.n_layer = 12\n",
        "            self.n_head = 8\n",
        "            self.n_dims = 20\n",
        "            self.n_positions = 101\n",
        "\n",
        "    class training_c():\n",
        "        class curriculum_c():\n",
        "            class dims_c():\n",
        "                def __init__(self):     # dims\n",
        "                    self.start = 5\n",
        "                    self.end = 20\n",
        "                    self.inc = 1\n",
        "                    self.interval = 2000\n",
        "\n",
        "            class points_c():\n",
        "                def __init__(self):     # points\n",
        "                    self.start = 11\n",
        "                    self.end = 41\n",
        "                    self.inc = 2\n",
        "                    self.interval = 2000\n",
        "\n",
        "            def __init__(self):         # curriculum\n",
        "                self.dims = self.dims_c()\n",
        "                self.points = self.points_c()\n",
        "\n",
        "        def __init__(self):             # training\n",
        "            self.task = \"linear_regression\"                 #  \"linear_regression\", \"sparse_linear_regression\", \"linear_classification\", \"relu_2nn_regression\", \"decision_tree\"\n",
        "            self.data = \"gaussian\"\n",
        "            self.task_kwargs = {}\n",
        "            self.num_tasks = None\n",
        "            self.num_training_examples = None\n",
        "            self.batch_size = 64\n",
        "            self.learning_rate = .0001\n",
        "            self.save_every_steps = 1000\n",
        "            self.keep_every_steps = 100000\n",
        "            self.train_steps = 500001\n",
        "            self.resume_id = None\n",
        "\n",
        "            self.curriculum = self.curriculum_c()\n",
        "\n",
        "    def __init__(self):                 # args\n",
        "        self.wandb = self.wandb_c()\n",
        "        self.model = self.model_c()\n",
        "        self.training = self.training_c()\n",
        "\n",
        "        self.out_dir = \"./models/test\"\n",
        "        self.test_run = False\n",
        "        self.using_wandb = False\n",
        "\n",
        "args = args_c()"
      ],
      "metadata": {
        "id": "T41GRct3BfCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "x4xotR5ai-0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import GPT2Model, GPT2Config\n",
        "from tqdm import tqdm\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression, Lasso\n",
        "import warnings\n",
        "from sklearn import tree\n",
        "import xgboost as xgb"
      ],
      "metadata": {
        "id": "8qZ_eCxA5hin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(conf):\n",
        "    if conf.family == \"gpt2\":\n",
        "        model = TransformerModel(\n",
        "            n_dims=conf.n_dims,\n",
        "            n_positions=conf.n_positions,\n",
        "            n_embd=conf.n_embd,\n",
        "            n_layer=conf.n_layer,\n",
        "            n_head=conf.n_head,\n",
        "        )\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_relevant_baselines(task_name):\n",
        "    task_to_baselines = {\n",
        "        \"linear_regression\": [\n",
        "            (LeastSquaresModel, {}),\n",
        "            (NNModel, {\"n_neighbors\": 3}),\n",
        "            (AveragingModel, {}),\n",
        "        ],\n",
        "        \"linear_classification\": [\n",
        "            (NNModel, {\"n_neighbors\": 3}),\n",
        "            (AveragingModel, {}),\n",
        "        ],\n",
        "        \"sparse_linear_regression\": [\n",
        "            (LeastSquaresModel, {}),\n",
        "            (NNModel, {\"n_neighbors\": 3}),\n",
        "            (AveragingModel, {}),\n",
        "        ]\n",
        "        + [(LassoModel, {\"alpha\": alpha}) for alpha in [1, 0.1, 0.01, 0.001, 0.0001]],\n",
        "        \"relu_2nn_regression\": [\n",
        "            (LeastSquaresModel, {}),\n",
        "            (NNModel, {\"n_neighbors\": 3}),\n",
        "            (AveragingModel, {}),\n",
        "            (\n",
        "                GDModel,\n",
        "                {\n",
        "                    \"model_class\": NeuralNetwork,\n",
        "                    \"model_class_args\": {\n",
        "                        \"in_size\": 20,\n",
        "                        \"hidden_size\": 100,\n",
        "                        \"out_size\": 1,\n",
        "                    },\n",
        "                    \"opt_alg\": \"adam\",\n",
        "                    \"batch_size\": 100,\n",
        "                    \"lr\": 5e-3,\n",
        "                    \"num_steps\": 100,\n",
        "                },\n",
        "            ),\n",
        "        ],\n",
        "        \"decision_tree\": [\n",
        "            (LeastSquaresModel, {}),\n",
        "            (NNModel, {\"n_neighbors\": 3}),\n",
        "            (DecisionTreeModel, {\"max_depth\": 4}),\n",
        "            (DecisionTreeModel, {\"max_depth\": None}),\n",
        "            (XGBoostModel, {}),\n",
        "            (AveragingModel, {}),\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    models = [model_cls(**kwargs) for model_cls, kwargs in task_to_baselines[task_name]]\n",
        "    return models\n",
        "\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, in_size=50, hidden_size=1000, out_size=1):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, out_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.net(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ParallelNetworks(nn.Module):\n",
        "    def __init__(self, num_models, model_class, **model_class_init_args):\n",
        "        super(ParallelNetworks, self).__init__()\n",
        "        self.nets = nn.ModuleList(\n",
        "            [model_class(**model_class_init_args) for i in range(num_models)]\n",
        "        )\n",
        "\n",
        "    def forward(self, xs):\n",
        "        assert xs.shape[0] == len(self.nets)\n",
        "\n",
        "        for i in range(len(self.nets)):\n",
        "            out = self.nets[i](xs[i])\n",
        "            if i == 0:\n",
        "                outs = torch.zeros(\n",
        "                    [len(self.nets)] + list(out.shape), device=out.device\n",
        "                )\n",
        "            outs[i] = out\n",
        "        return outs\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, n_dims, n_positions, n_embd=128, n_layer=12, n_head=4):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        configuration = GPT2Config(\n",
        "            n_positions=2 * n_positions,\n",
        "            n_embd=n_embd,\n",
        "            n_layer=n_layer,\n",
        "            n_head=n_head,\n",
        "            resid_pdrop=0.0,\n",
        "            embd_pdrop=0.0,\n",
        "            attn_pdrop=0.0,\n",
        "            use_cache=False,\n",
        "        )\n",
        "        self.name = f\"gpt2_embd={n_embd}_layer={n_layer}_head={n_head}\"\n",
        "\n",
        "        self.n_positions = n_positions\n",
        "        self.n_dims = n_dims\n",
        "        self._read_in = nn.Linear(n_dims, n_embd)\n",
        "        self._backbone = GPT2Model(configuration)\n",
        "        self._read_out = nn.Linear(n_embd, 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def _combine(xs_b, ys_b):\n",
        "        \"\"\"Interleaves the x's and the y's into a single sequence.\"\"\"\n",
        "        bsize, points, dim = xs_b.shape\n",
        "        ys_b_wide = torch.cat(\n",
        "            (\n",
        "                ys_b.view(bsize, points, 1),\n",
        "                torch.zeros(bsize, points, dim - 1, device=ys_b.device),\n",
        "            ),\n",
        "            axis=2,\n",
        "        )\n",
        "        zs = torch.stack((xs_b, ys_b_wide), dim=2)\n",
        "        zs = zs.view(bsize, 2 * points, dim)\n",
        "        return zs\n",
        "\n",
        "    def forward(self, xs, ys, inds=None):\n",
        "        if inds is None:\n",
        "            inds = torch.arange(ys.shape[1])\n",
        "        else:\n",
        "            inds = torch.tensor(inds)\n",
        "            if max(inds) >= ys.shape[1] or min(inds) < 0:\n",
        "                raise ValueError(\"inds contain indices where xs and ys are not defined\")\n",
        "        zs = self._combine(xs, ys)\n",
        "        embeds = self._read_in(zs)\n",
        "        output = self._backbone(inputs_embeds=embeds).last_hidden_state\n",
        "        prediction = self._read_out(output)\n",
        "        return prediction[:, ::2, 0][:, inds]  # predict only on xs\n",
        "\n",
        "\n",
        "class NNModel:\n",
        "    def __init__(self, n_neighbors, weights=\"uniform\"):\n",
        "        # should we be picking k optimally\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.weights = weights\n",
        "        self.name = f\"NN_n={n_neighbors}_{weights}\"\n",
        "\n",
        "    def __call__(self, xs, ys, inds=None):\n",
        "        if inds is None:\n",
        "            inds = range(ys.shape[1])\n",
        "        else:\n",
        "            if max(inds) >= ys.shape[1] or min(inds) < 0:\n",
        "                raise ValueError(\"inds contain indices where xs and ys are not defined\")\n",
        "\n",
        "        preds = []\n",
        "\n",
        "        for i in inds:\n",
        "            if i == 0:\n",
        "                preds.append(torch.zeros_like(ys[:, 0]))  # predict zero for first point\n",
        "                continue\n",
        "            train_xs, train_ys = xs[:, :i], ys[:, :i]\n",
        "            test_x = xs[:, i : i + 1]\n",
        "            dist = (train_xs - test_x).square().sum(dim=2).sqrt()\n",
        "\n",
        "            if self.weights == \"uniform\":\n",
        "                weights = torch.ones_like(dist)\n",
        "            else:\n",
        "                weights = 1.0 / dist\n",
        "                inf_mask = torch.isinf(weights).float()  # deal with exact match\n",
        "                inf_row = torch.any(inf_mask, axis=1)\n",
        "                weights[inf_row] = inf_mask[inf_row]\n",
        "\n",
        "            pred = []\n",
        "            k = min(i, self.n_neighbors)\n",
        "            ranks = dist.argsort()[:, :k]\n",
        "            for y, w, n in zip(train_ys, weights, ranks):\n",
        "                y, w = y[n], w[n]\n",
        "                pred.append((w * y).sum() / w.sum())\n",
        "            preds.append(torch.stack(pred))\n",
        "\n",
        "        return torch.stack(preds, dim=1)\n",
        "\n",
        "\n",
        "# xs and ys should be on cpu for this method. Otherwise the output maybe off in case when train_xs is not full rank due to the implementation of torch.linalg.lstsq.\n",
        "class LeastSquaresModel:\n",
        "    def __init__(self, driver=None):\n",
        "        self.driver = driver\n",
        "        self.name = f\"OLS_driver={driver}\"\n",
        "\n",
        "    def __call__(self, xs, ys, inds=None):\n",
        "        xs, ys = xs.cpu(), ys.cpu()\n",
        "        if inds is None:\n",
        "            inds = range(ys.shape[1])\n",
        "        else:\n",
        "            if max(inds) >= ys.shape[1] or min(inds) < 0:\n",
        "                raise ValueError(\"inds contain indices where xs and ys are not defined\")\n",
        "\n",
        "        preds = []\n",
        "\n",
        "        for i in inds:\n",
        "            if i == 0:\n",
        "                preds.append(torch.zeros_like(ys[:, 0]))  # predict zero for first point\n",
        "                continue\n",
        "            train_xs, train_ys = xs[:, :i], ys[:, :i]\n",
        "            test_x = xs[:, i : i + 1]\n",
        "\n",
        "            ws, _, _, _ = torch.linalg.lstsq(\n",
        "                train_xs, train_ys.unsqueeze(2), driver=self.driver\n",
        "            )\n",
        "\n",
        "            pred = test_x @ ws\n",
        "            preds.append(pred[:, 0, 0])\n",
        "\n",
        "        return torch.stack(preds, dim=1)\n",
        "\n",
        "\n",
        "class AveragingModel:\n",
        "    def __init__(self):\n",
        "        self.name = \"averaging\"\n",
        "\n",
        "    def __call__(self, xs, ys, inds=None):\n",
        "        if inds is None:\n",
        "            inds = range(ys.shape[1])\n",
        "        else:\n",
        "            if max(inds) >= ys.shape[1] or min(inds) < 0:\n",
        "                raise ValueError(\"inds contain indices where xs and ys are not defined\")\n",
        "\n",
        "        preds = []\n",
        "\n",
        "        for i in inds:\n",
        "            if i == 0:\n",
        "                preds.append(torch.zeros_like(ys[:, 0]))  # predict zero for first point\n",
        "                continue\n",
        "            train_xs, train_ys = xs[:, :i], ys[:, :i]\n",
        "            test_x = xs[:, i : i + 1]\n",
        "\n",
        "            train_zs = train_xs * train_ys.unsqueeze(dim=-1)\n",
        "            w_p = train_zs.mean(dim=1).unsqueeze(dim=-1)\n",
        "            pred = test_x @ w_p\n",
        "            preds.append(pred[:, 0, 0])\n",
        "\n",
        "        return torch.stack(preds, dim=1)\n",
        "\n",
        "\n",
        "# Lasso regression (for sparse linear regression).\n",
        "# Seems to take more time as we decrease alpha.\n",
        "class LassoModel:\n",
        "    def __init__(self, alpha, max_iter=100000):\n",
        "        # the l1 regularizer gets multiplied by alpha.\n",
        "        self.alpha = alpha\n",
        "        self.max_iter = max_iter\n",
        "        self.name = f\"lasso_alpha={alpha}_max_iter={max_iter}\"\n",
        "\n",
        "    # inds is a list containing indices where we want the prediction.\n",
        "    # prediction made at all indices by default.\n",
        "    def __call__(self, xs, ys, inds=None):\n",
        "        xs, ys = xs.cpu(), ys.cpu()\n",
        "\n",
        "        if inds is None:\n",
        "            inds = range(ys.shape[1])\n",
        "        else:\n",
        "            if max(inds) >= ys.shape[1] or min(inds) < 0:\n",
        "                raise ValueError(\"inds contain indices where xs and ys are not defined\")\n",
        "\n",
        "        preds = []  # predict one for first point\n",
        "\n",
        "        # i: loop over num_points\n",
        "        # j: loop over bsize\n",
        "        for i in inds:\n",
        "            pred = torch.zeros_like(ys[:, 0])\n",
        "\n",
        "            if i > 0:\n",
        "                pred = torch.zeros_like(ys[:, 0])\n",
        "                for j in range(ys.shape[0]):\n",
        "                    train_xs, train_ys = xs[j, :i], ys[j, :i]\n",
        "\n",
        "                    # If all points till now have the same label, predict that label.\n",
        "\n",
        "                    clf = Lasso(\n",
        "                        alpha=self.alpha, fit_intercept=False, max_iter=self.max_iter\n",
        "                    )\n",
        "\n",
        "                    # Check for convergence.\n",
        "                    with warnings.catch_warnings():\n",
        "                        warnings.filterwarnings(\"error\")\n",
        "                        try:\n",
        "                            clf.fit(train_xs, train_ys)\n",
        "                        except Warning:\n",
        "                            print(f\"lasso convergence warning at i={i}, j={j}.\")\n",
        "                            raise\n",
        "\n",
        "                    w_pred = torch.from_numpy(clf.coef_).unsqueeze(1)\n",
        "\n",
        "                    test_x = xs[j, i : i + 1]\n",
        "                    y_pred = (test_x @ w_pred.float()).squeeze(1)\n",
        "                    pred[j] = y_pred[0]\n",
        "\n",
        "            preds.append(pred)\n",
        "\n",
        "        return torch.stack(preds, dim=1)\n",
        "\n",
        "\n",
        "# Gradient Descent and variants.\n",
        "# Example usage: gd_model = GDModel(NeuralNetwork, {'in_size': 50, 'hidden_size':400, 'out_size' :1}, opt_alg = 'adam', batch_size = 100, lr = 5e-3, num_steps = 200)\n",
        "class GDModel:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_class,\n",
        "        model_class_args,\n",
        "        opt_alg=\"sgd\",\n",
        "        batch_size=1,\n",
        "        num_steps=1000,\n",
        "        lr=1e-3,\n",
        "        loss_name=\"squared\",\n",
        "    ):\n",
        "        # model_class: torch.nn model class\n",
        "        # model_class_args: a dict containing arguments for model_class\n",
        "        # opt_alg can be 'sgd' or 'adam'\n",
        "        # verbose: whether to print the progress or not\n",
        "        # batch_size: batch size for sgd\n",
        "        self.model_class = model_class\n",
        "        self.model_class_args = model_class_args\n",
        "        self.opt_alg = opt_alg\n",
        "        self.lr = lr\n",
        "        self.batch_size = batch_size\n",
        "        self.num_steps = num_steps\n",
        "        self.loss_name = loss_name\n",
        "\n",
        "        self.name = f\"gd_model_class={model_class}_model_class_args={model_class_args}_opt_alg={opt_alg}_lr={lr}_batch_size={batch_size}_num_steps={num_steps}_loss_name={loss_name}\"\n",
        "\n",
        "    def __call__(self, xs, ys, inds=None, verbose=False, print_step=100):\n",
        "        # inds is a list containing indices where we want the prediction.\n",
        "        # prediction made at all indices by default.\n",
        "        # xs: bsize X npoints X ndim.\n",
        "        # ys: bsize X npoints.\n",
        "        xs, ys = xs.cuda(), ys.cuda()\n",
        "\n",
        "        if inds is None:\n",
        "            inds = range(ys.shape[1])\n",
        "        else:\n",
        "            if max(inds) >= ys.shape[1] or min(inds) < 0:\n",
        "                raise ValueError(\"inds contain indices where xs and ys are not defined\")\n",
        "\n",
        "        preds = []  # predict one for first point\n",
        "\n",
        "        # i: loop over num_points\n",
        "        for i in tqdm(inds):\n",
        "            pred = torch.zeros_like(ys[:, 0])\n",
        "            model = ParallelNetworks(\n",
        "                ys.shape[0], self.model_class, **self.model_class_args\n",
        "            )\n",
        "            model.cuda()\n",
        "            if i > 0:\n",
        "                pred = torch.zeros_like(ys[:, 0])\n",
        "\n",
        "                train_xs, train_ys = xs[:, :i], ys[:, :i]\n",
        "                test_xs, test_ys = xs[:, i : i + 1], ys[:, i : i + 1]\n",
        "\n",
        "                if self.opt_alg == \"sgd\":\n",
        "                    optimizer = torch.optim.SGD(model.parameters(), lr=self.lr)\n",
        "                elif self.opt_alg == \"adam\":\n",
        "                    optimizer = torch.optim.Adam(model.parameters(), lr=self.lr)\n",
        "                else:\n",
        "                    raise NotImplementedError(f\"{self.opt_alg} not implemented.\")\n",
        "\n",
        "                if self.loss_name == \"squared\":\n",
        "                    loss_criterion = nn.MSELoss()\n",
        "                else:\n",
        "                    raise NotImplementedError(f\"{self.loss_name} not implemented.\")\n",
        "\n",
        "                # Training loop\n",
        "                for j in range(self.num_steps):\n",
        "\n",
        "                    # Prepare batch\n",
        "                    mask = torch.zeros(i).bool()\n",
        "                    perm = torch.randperm(i)\n",
        "                    mask[perm[: self.batch_size]] = True\n",
        "                    train_xs_cur, train_ys_cur = train_xs[:, mask, :], train_ys[:, mask]\n",
        "\n",
        "                    if verbose and j % print_step == 0:\n",
        "                        model.eval()\n",
        "                        with torch.no_grad():\n",
        "                            outputs = model(train_xs_cur)\n",
        "                            loss = loss_criterion(\n",
        "                                outputs[:, :, 0], train_ys_cur\n",
        "                            ).detach()\n",
        "                            outputs_test = model(test_xs)\n",
        "                            test_loss = loss_criterion(\n",
        "                                outputs_test[:, :, 0], test_ys\n",
        "                            ).detach()\n",
        "                            print(\n",
        "                                f\"ind:{i},step:{j}, train_loss:{loss.item()}, test_loss:{test_loss.item()}\"\n",
        "                            )\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    model.train()\n",
        "                    outputs = model(train_xs_cur)\n",
        "                    loss = loss_criterion(outputs[:, :, 0], train_ys_cur)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                model.eval()\n",
        "                pred = model(test_xs).detach()\n",
        "\n",
        "                assert pred.shape[1] == 1 and pred.shape[2] == 1\n",
        "                pred = pred[:, 0, 0]\n",
        "\n",
        "            preds.append(pred)\n",
        "\n",
        "        return torch.stack(preds, dim=1)\n",
        "\n",
        "\n",
        "class DecisionTreeModel:\n",
        "    def __init__(self, max_depth=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.name = f\"decision_tree_max_depth={max_depth}\"\n",
        "\n",
        "    # inds is a list containing indices where we want the prediction.\n",
        "    # prediction made at all indices by default.\n",
        "    def __call__(self, xs, ys, inds=None):\n",
        "        xs, ys = xs.cpu(), ys.cpu()\n",
        "\n",
        "        if inds is None:\n",
        "            inds = range(ys.shape[1])\n",
        "        else:\n",
        "            if max(inds) >= ys.shape[1] or min(inds) < 0:\n",
        "                raise ValueError(\"inds contain indices where xs and ys are not defined\")\n",
        "\n",
        "        preds = []\n",
        "\n",
        "        # i: loop over num_points\n",
        "        # j: loop over bsize\n",
        "        for i in inds:\n",
        "            pred = torch.zeros_like(ys[:, 0])\n",
        "\n",
        "            if i > 0:\n",
        "                pred = torch.zeros_like(ys[:, 0])\n",
        "                for j in range(ys.shape[0]):\n",
        "                    train_xs, train_ys = xs[j, :i], ys[j, :i]\n",
        "\n",
        "                    clf = tree.DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "                    clf = clf.fit(train_xs, train_ys)\n",
        "                    test_x = xs[j, i : i + 1]\n",
        "                    y_pred = clf.predict(test_x)\n",
        "                    pred[j] = y_pred[0]\n",
        "\n",
        "            preds.append(pred)\n",
        "\n",
        "        return torch.stack(preds, dim=1)\n",
        "\n",
        "\n",
        "class XGBoostModel:\n",
        "    def __init__(self):\n",
        "        self.name = \"xgboost\"\n",
        "\n",
        "    # inds is a list containing indices where we want the prediction.\n",
        "    # prediction made at all indices by default.\n",
        "    def __call__(self, xs, ys, inds=None):\n",
        "        xs, ys = xs.cpu(), ys.cpu()\n",
        "\n",
        "        if inds is None:\n",
        "            inds = range(ys.shape[1])\n",
        "        else:\n",
        "            if max(inds) >= ys.shape[1] or min(inds) < 0:\n",
        "                raise ValueError(\"inds contain indices where xs and ys are not defined\")\n",
        "\n",
        "        preds = []\n",
        "\n",
        "        # i: loop over num_points\n",
        "        # j: loop over bsize\n",
        "        for i in tqdm(inds):\n",
        "            pred = torch.zeros_like(ys[:, 0])\n",
        "            if i > 0:\n",
        "                pred = torch.zeros_like(ys[:, 0])\n",
        "                for j in range(ys.shape[0]):\n",
        "                    train_xs, train_ys = xs[j, :i], ys[j, :i]\n",
        "\n",
        "                    clf = xgb.XGBRegressor()\n",
        "\n",
        "                    clf = clf.fit(train_xs, train_ys)\n",
        "                    test_x = xs[j, i : i + 1]\n",
        "                    y_pred = clf.predict(test_x)\n",
        "                    pred[j] = y_pred[0].item()\n",
        "\n",
        "            preds.append(pred)\n",
        "\n",
        "        return torch.stack(preds, dim=1)"
      ],
      "metadata": {
        "id": "25rFZAws0Fsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Samplers\n"
      ],
      "metadata": {
        "id": "JcP-QF8BDpDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import torch"
      ],
      "metadata": {
        "id": "0QzD2i99D8_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataSampler:\n",
        "    def __init__(self, n_dims):\n",
        "        self.n_dims = n_dims\n",
        "\n",
        "    def sample_xs(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "def get_data_sampler(data_name, n_dims, **kwargs):\n",
        "    names_to_classes = {\n",
        "        \"gaussian\": GaussianSampler,\n",
        "    }\n",
        "    if data_name in names_to_classes:\n",
        "        sampler_cls = names_to_classes[data_name]\n",
        "        return sampler_cls(n_dims, **kwargs)\n",
        "    else:\n",
        "        print(\"Unknown sampler\")\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "def sample_transformation(eigenvalues, normalize=False):\n",
        "    n_dims = len(eigenvalues)\n",
        "    U, _, _ = torch.linalg.svd(torch.randn(n_dims, n_dims))\n",
        "    t = U @ torch.diag(eigenvalues) @ torch.transpose(U, 0, 1)\n",
        "    if normalize:\n",
        "        norm_subspace = torch.sum(eigenvalues**2)\n",
        "        t *= math.sqrt(n_dims / norm_subspace)\n",
        "    return t\n",
        "\n",
        "\n",
        "class GaussianSampler(DataSampler):\n",
        "    def __init__(self, n_dims, bias=None, scale=None):\n",
        "        super().__init__(n_dims)\n",
        "        self.bias = bias\n",
        "        self.scale = scale\n",
        "\n",
        "    def sample_xs(self, n_points, b_size, n_dims_truncated=None, seeds=None):\n",
        "        if seeds is None:\n",
        "            xs_b = torch.randn(b_size, n_points, self.n_dims)\n",
        "        else:\n",
        "            xs_b = torch.zeros(b_size, n_points, self.n_dims)\n",
        "            generator = torch.Generator()\n",
        "            assert len(seeds) == b_size\n",
        "            for i, seed in enumerate(seeds):\n",
        "                generator.manual_seed(seed)\n",
        "                xs_b[i] = torch.randn(n_points, self.n_dims, generator=generator)\n",
        "        if self.scale is not None:\n",
        "            xs_b = xs_b @ self.scale\n",
        "        if self.bias is not None:\n",
        "            xs_b += self.bias\n",
        "        if n_dims_truncated is not None:\n",
        "            xs_b[:, :, n_dims_truncated:] = 0\n",
        "        return xs_b"
      ],
      "metadata": {
        "id": "aNuGvNB1Dqdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tasks"
      ],
      "metadata": {
        "id": "xo9Lg1DMl4pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import torch"
      ],
      "metadata": {
        "id": "GVy1xbStEUmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def squared_error(ys_pred, ys):\n",
        "    return (ys - ys_pred).square()\n",
        "\n",
        "\n",
        "def mean_squared_error(ys_pred, ys):\n",
        "    return (ys - ys_pred).square().mean()\n",
        "\n",
        "\n",
        "def accuracy(ys_pred, ys):\n",
        "    return (ys == ys_pred.sign()).float()\n",
        "\n",
        "\n",
        "sigmoid = torch.nn.Sigmoid()\n",
        "bce_loss = torch.nn.BCELoss()\n",
        "\n",
        "\n",
        "def cross_entropy(ys_pred, ys):\n",
        "    output = sigmoid(ys_pred)\n",
        "    target = (ys + 1) / 2\n",
        "    return bce_loss(output, target)\n",
        "\n",
        "\n",
        "class Task:\n",
        "    def __init__(self, n_dims, batch_size, pool_dict=None, seeds=None):\n",
        "        self.n_dims = n_dims\n",
        "        self.b_size = batch_size\n",
        "        self.pool_dict = pool_dict\n",
        "        self.seeds = seeds\n",
        "        assert pool_dict is None or seeds is None\n",
        "\n",
        "    def evaluate(self, xs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_pool_dict(n_dims, num_tasks):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @staticmethod\n",
        "    def get_metric():\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @staticmethod\n",
        "    def get_training_metric():\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "def get_task_sampler(\n",
        "    task_name, n_dims, batch_size, pool_dict=None, num_tasks=None, **kwargs\n",
        "):\n",
        "    task_names_to_classes = {\n",
        "        \"linear_regression\": LinearRegression,\n",
        "        \"sparse_linear_regression\": SparseLinearRegression,\n",
        "        \"linear_classification\": LinearClassification,\n",
        "        \"noisy_linear_regression\": NoisyLinearRegression,\n",
        "        \"quadratic_regression\": QuadraticRegression,\n",
        "        \"relu_2nn_regression\": Relu2nnRegression,\n",
        "        \"decision_tree\": DecisionTree,\n",
        "    }\n",
        "    if task_name in task_names_to_classes:\n",
        "        task_cls = task_names_to_classes[task_name]\n",
        "        if num_tasks is not None:\n",
        "            if pool_dict is not None:\n",
        "                raise ValueError(\"Either pool_dict or num_tasks should be None.\")\n",
        "            pool_dict = task_cls.generate_pool_dict(n_dims, num_tasks, **kwargs)\n",
        "        return lambda **args: task_cls(n_dims, batch_size, pool_dict, **args, **kwargs)\n",
        "    else:\n",
        "        print(\"Unknown task\")\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class LinearRegression(Task):\n",
        "    def __init__(self, n_dims, batch_size, pool_dict=None, seeds=None, scale=1):\n",
        "        \"\"\"scale: a constant by which to scale the randomly sampled weights.\"\"\"\n",
        "        super(LinearRegression, self).__init__(n_dims, batch_size, pool_dict, seeds)\n",
        "        self.scale = scale\n",
        "\n",
        "        if pool_dict is None and seeds is None:\n",
        "            self.w_b = torch.randn(self.b_size, self.n_dims, 1)\n",
        "        elif seeds is not None:\n",
        "            self.w_b = torch.zeros(self.b_size, self.n_dims, 1)\n",
        "            generator = torch.Generator()\n",
        "            assert len(seeds) == self.b_size\n",
        "            for i, seed in enumerate(seeds):\n",
        "                generator.manual_seed(seed)\n",
        "                self.w_b[i] = torch.randn(self.n_dims, 1, generator=generator)\n",
        "        else:\n",
        "            assert \"w\" in pool_dict\n",
        "            indices = torch.randperm(len(pool_dict[\"w\"]))[:batch_size]\n",
        "            self.w_b = pool_dict[\"w\"][indices]\n",
        "\n",
        "    def evaluate(self, xs_b):\n",
        "        w_b = self.w_b.to(xs_b.device)\n",
        "        ys_b = self.scale * (xs_b @ w_b)[:, :, 0]\n",
        "        return ys_b\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_pool_dict(n_dims, num_tasks, **kwargs):  # ignore extra args\n",
        "        return {\"w\": torch.randn(num_tasks, n_dims, 1)}\n",
        "\n",
        "    @staticmethod\n",
        "    def get_metric():\n",
        "        return squared_error\n",
        "\n",
        "    @staticmethod\n",
        "    def get_training_metric():\n",
        "        return mean_squared_error\n",
        "\n",
        "\n",
        "class SparseLinearRegression(LinearRegression):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_dims,\n",
        "        batch_size,\n",
        "        pool_dict=None,\n",
        "        seeds=None,\n",
        "        scale=1,\n",
        "        sparsity=3,\n",
        "        valid_coords=None,\n",
        "    ):\n",
        "        \"\"\"scale: a constant by which to scale the randomly sampled weights.\"\"\"\n",
        "        super(SparseLinearRegression, self).__init__(\n",
        "            n_dims, batch_size, pool_dict, seeds, scale\n",
        "        )\n",
        "        self.sparsity = sparsity\n",
        "        if valid_coords is None:\n",
        "            valid_coords = n_dims\n",
        "        assert valid_coords <= n_dims\n",
        "\n",
        "        for i, w in enumerate(self.w_b):\n",
        "            mask = torch.ones(n_dims).bool()\n",
        "            if seeds is None:\n",
        "                perm = torch.randperm(valid_coords)\n",
        "            else:\n",
        "                generator = torch.Generator()\n",
        "                generator.manual_seed(seeds[i])\n",
        "                perm = torch.randperm(valid_coords, generator=generator)\n",
        "            mask[perm[:sparsity]] = False\n",
        "            w[mask] = 0\n",
        "\n",
        "    def evaluate(self, xs_b):\n",
        "        w_b = self.w_b.to(xs_b.device)\n",
        "        ys_b = self.scale * (xs_b @ w_b)[:, :, 0]\n",
        "        return ys_b\n",
        "\n",
        "    @staticmethod\n",
        "    def get_metric():\n",
        "        return squared_error\n",
        "\n",
        "    @staticmethod\n",
        "    def get_training_metric():\n",
        "        return mean_squared_error\n",
        "\n",
        "\n",
        "class LinearClassification(LinearRegression):\n",
        "    def evaluate(self, xs_b):\n",
        "        ys_b = super().evaluate(xs_b)\n",
        "        return ys_b.sign()\n",
        "\n",
        "    @staticmethod\n",
        "    def get_metric():\n",
        "        return accuracy\n",
        "\n",
        "    @staticmethod\n",
        "    def get_training_metric():\n",
        "        return cross_entropy\n",
        "\n",
        "\n",
        "class NoisyLinearRegression(LinearRegression):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_dims,\n",
        "        batch_size,\n",
        "        pool_dict=None,\n",
        "        seeds=None,\n",
        "        scale=1,\n",
        "        noise_std=0,\n",
        "        renormalize_ys=False,\n",
        "    ):\n",
        "        \"\"\"noise_std: standard deviation of noise added to the prediction.\"\"\"\n",
        "        super(NoisyLinearRegression, self).__init__(\n",
        "            n_dims, batch_size, pool_dict, seeds, scale\n",
        "        )\n",
        "        self.noise_std = noise_std\n",
        "        self.renormalize_ys = renormalize_ys\n",
        "\n",
        "    def evaluate(self, xs_b):\n",
        "        ys_b = super().evaluate(xs_b)\n",
        "        ys_b_noisy = ys_b + torch.randn_like(ys_b) * self.noise_std\n",
        "        if self.renormalize_ys:\n",
        "            ys_b_noisy = ys_b_noisy * math.sqrt(self.n_dims) / ys_b_noisy.std()\n",
        "\n",
        "        return ys_b_noisy\n",
        "\n",
        "\n",
        "class QuadraticRegression(LinearRegression):\n",
        "    def evaluate(self, xs_b):\n",
        "        w_b = self.w_b.to(xs_b.device)\n",
        "        ys_b_quad = ((xs_b**2) @ w_b)[:, :, 0]\n",
        "        #         ys_b_quad = ys_b_quad * math.sqrt(self.n_dims) / ys_b_quad.std()\n",
        "        # Renormalize to Linear Regression Scale\n",
        "        ys_b_quad = ys_b_quad / math.sqrt(3)\n",
        "        ys_b_quad = self.scale * ys_b_quad\n",
        "        return ys_b_quad\n",
        "\n",
        "\n",
        "class Relu2nnRegression(Task):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_dims,\n",
        "        batch_size,\n",
        "        pool_dict=None,\n",
        "        seeds=None,\n",
        "        scale=1,\n",
        "        hidden_layer_size=100,\n",
        "    ):\n",
        "        \"\"\"scale: a constant by which to scale the randomly sampled weights.\"\"\"\n",
        "        super(Relu2nnRegression, self).__init__(n_dims, batch_size, pool_dict, seeds)\n",
        "        self.scale = scale\n",
        "        self.hidden_layer_size = hidden_layer_size\n",
        "\n",
        "        if pool_dict is None and seeds is None:\n",
        "            self.W1 = torch.randn(self.b_size, self.n_dims, hidden_layer_size)\n",
        "            self.W2 = torch.randn(self.b_size, hidden_layer_size, 1)\n",
        "        elif seeds is not None:\n",
        "            self.W1 = torch.zeros(self.b_size, self.n_dims, hidden_layer_size)\n",
        "            self.W2 = torch.zeros(self.b_size, hidden_layer_size, 1)\n",
        "            generator = torch.Generator()\n",
        "            assert len(seeds) == self.b_size\n",
        "            for i, seed in enumerate(seeds):\n",
        "                generator.manual_seed(seed)\n",
        "                self.W1[i] = torch.randn(\n",
        "                    self.n_dims, hidden_layer_size, generator=generator\n",
        "                )\n",
        "                self.W2[i] = torch.randn(hidden_layer_size, 1, generator=generator)\n",
        "        else:\n",
        "            assert \"W1\" in pool_dict and \"W2\" in pool_dict\n",
        "            assert len(pool_dict[\"W1\"]) == len(pool_dict[\"W2\"])\n",
        "            indices = torch.randperm(len(pool_dict[\"W1\"]))[:batch_size]\n",
        "            self.W1 = pool_dict[\"W1\"][indices]\n",
        "            self.W2 = pool_dict[\"W2\"][indices]\n",
        "\n",
        "    def evaluate(self, xs_b):\n",
        "        W1 = self.W1.to(xs_b.device)\n",
        "        W2 = self.W2.to(xs_b.device)\n",
        "        # Renormalize to Linear Regression Scale\n",
        "        ys_b_nn = (torch.nn.functional.relu(xs_b @ W1) @ W2)[:, :, 0]\n",
        "        ys_b_nn = ys_b_nn * math.sqrt(2 / self.hidden_layer_size)\n",
        "        ys_b_nn = self.scale * ys_b_nn\n",
        "        #         ys_b_nn = ys_b_nn * math.sqrt(self.n_dims) / ys_b_nn.std()\n",
        "        return ys_b_nn\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_pool_dict(n_dims, num_tasks, hidden_layer_size=4, **kwargs):\n",
        "        return {\n",
        "            \"W1\": torch.randn(num_tasks, n_dims, hidden_layer_size),\n",
        "            \"W2\": torch.randn(num_tasks, hidden_layer_size, 1),\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def get_metric():\n",
        "        return squared_error\n",
        "\n",
        "    @staticmethod\n",
        "    def get_training_metric():\n",
        "        return mean_squared_error\n",
        "\n",
        "\n",
        "class DecisionTree(Task):\n",
        "    def __init__(self, n_dims, batch_size, pool_dict=None, seeds=None, depth=4):\n",
        "\n",
        "        super(DecisionTree, self).__init__(n_dims, batch_size, pool_dict, seeds)\n",
        "        self.depth = depth\n",
        "\n",
        "        if pool_dict is None:\n",
        "\n",
        "            # We represent the tree using an array (tensor). Root node is at index 0, its 2 children at index 1 and 2...\n",
        "            # dt_tensor stores the coordinate used at each node of the decision tree.\n",
        "            # Only indices corresponding to non-leaf nodes are relevant\n",
        "            self.dt_tensor = torch.randint(\n",
        "                low=0, high=n_dims, size=(batch_size, 2 ** (depth + 1) - 1)\n",
        "            )\n",
        "\n",
        "            # Target value at the leaf nodes.\n",
        "            # Only indices corresponding to leaf nodes are relevant.\n",
        "            self.target_tensor = torch.randn(self.dt_tensor.shape)\n",
        "        elif seeds is not None:\n",
        "            self.dt_tensor = torch.zeros(batch_size, 2 ** (depth + 1) - 1)\n",
        "            self.target_tensor = torch.zeros_like(dt_tensor)\n",
        "            generator = torch.Generator()\n",
        "            assert len(seeds) == self.b_size\n",
        "            for i, seed in enumerate(seeds):\n",
        "                generator.manual_seed(seed)\n",
        "                self.dt_tensor[i] = torch.randint(\n",
        "                    low=0,\n",
        "                    high=n_dims - 1,\n",
        "                    size=2 ** (depth + 1) - 1,\n",
        "                    generator=generator,\n",
        "                )\n",
        "                self.target_tensor[i] = torch.randn(\n",
        "                    self.dt_tensor[i].shape, generator=generator\n",
        "                )\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def evaluate(self, xs_b):\n",
        "        dt_tensor = self.dt_tensor.to(xs_b.device)\n",
        "        target_tensor = self.target_tensor.to(xs_b.device)\n",
        "        ys_b = torch.zeros(xs_b.shape[0], xs_b.shape[1], device=xs_b.device)\n",
        "        for i in range(xs_b.shape[0]):\n",
        "            xs_bool = xs_b[i] > 0\n",
        "            # If a single decision tree present, use it for all the xs in the batch.\n",
        "            if self.b_size == 1:\n",
        "                dt = dt_tensor[0]\n",
        "                target = target_tensor[0]\n",
        "            else:\n",
        "                dt = dt_tensor[i]\n",
        "                target = target_tensor[i]\n",
        "\n",
        "            cur_nodes = torch.zeros(xs_b.shape[1], device=xs_b.device).long()\n",
        "            for j in range(self.depth):\n",
        "                cur_coords = dt[cur_nodes]\n",
        "                cur_decisions = xs_bool[torch.arange(xs_bool.shape[0]), cur_coords]\n",
        "                cur_nodes = 2 * cur_nodes + 1 + cur_decisions\n",
        "\n",
        "            ys_b[i] = target[cur_nodes]\n",
        "\n",
        "        return ys_b\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_pool_dict(n_dims, num_tasks, hidden_layer_size=4, **kwargs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @staticmethod\n",
        "    def get_metric():\n",
        "        return squared_error\n",
        "\n",
        "    @staticmethod\n",
        "    def get_training_metric():\n",
        "        return mean_squared_error"
      ],
      "metadata": {
        "id": "XL4LBg3_l7GA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Curriculum"
      ],
      "metadata": {
        "id": "iKUFD3nwEbz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "uO5HzxMTEfjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Curriculum:\n",
        "    def __init__(self, args):\n",
        "        # args.dims and args.points each contain start, end, inc, interval attributes\n",
        "        # inc denotes the change in n_dims,\n",
        "        # this change is done every interval,\n",
        "        # and start/end are the limits of the parameter\n",
        "        self.n_dims_truncated = args.dims.start\n",
        "        self.n_points = args.points.start\n",
        "        self.n_dims_schedule = args.dims\n",
        "        self.n_points_schedule = args.points\n",
        "        self.step_count = 0\n",
        "\n",
        "    def update(self):\n",
        "        self.step_count += 1\n",
        "        self.n_dims_truncated = self.update_var(\n",
        "            self.n_dims_truncated, self.n_dims_schedule\n",
        "        )\n",
        "        self.n_points = self.update_var(self.n_points, self.n_points_schedule)\n",
        "\n",
        "    def update_var(self, var, schedule):\n",
        "        if self.step_count % schedule.interval == 0:\n",
        "            var += schedule.inc\n",
        "\n",
        "        return min(var, schedule.end)\n",
        "\n",
        "\n",
        "# returns the final value of var after applying curriculum.\n",
        "def get_final_var(init_var, total_steps, inc, n_steps, lim):\n",
        "    final_var = init_var + math.floor((total_steps) / n_steps) * inc\n",
        "\n",
        "    return min(final_var, lim)"
      ],
      "metadata": {
        "id": "2NveV6WDEbXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation Metrics"
      ],
      "metadata": {
        "id": "YoTpHYaZE5OX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch"
      ],
      "metadata": {
        "id": "v0AFQsESE7fV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_from_run(run_path, step=-1, only_conf=False):\n",
        "    conf = args\n",
        "    if only_conf:\n",
        "        return None, conf\n",
        "\n",
        "    model = build_model(conf.model)\n",
        "\n",
        "    if step == -1:\n",
        "        state_path = os.path.join(run_path, \"state.pt\")\n",
        "        state = torch.load(state_path)\n",
        "        model.load_state_dict(state[\"model_state_dict\"])\n",
        "    else:\n",
        "        model_path = os.path.join(run_path, f\"model_{step}.pt\")\n",
        "        state_dict = torch.load(model_path)\n",
        "        model.load_state_dict(state_dict)\n",
        "\n",
        "    return model, conf\n",
        "\n",
        "\n",
        "# Functions for evaluation\n",
        "\n",
        "\n",
        "def eval_batch(model, task_sampler, xs, xs_p=None):\n",
        "    task = task_sampler()\n",
        "    if torch.cuda.is_available() and model.name.split(\"_\")[0] in [\"gpt2\", \"lstm\"]:\n",
        "        device = \"cuda\"\n",
        "    else:\n",
        "        device = \"cpu\"\n",
        "\n",
        "    if xs_p is None:\n",
        "        ys = task.evaluate(xs)\n",
        "        pred = model(xs.to(device), ys.to(device)).detach()\n",
        "        metrics = task.get_metric()(pred.cpu(), ys)\n",
        "    else:\n",
        "        b_size, n_points, _ = xs.shape\n",
        "        metrics = torch.zeros(b_size, n_points)\n",
        "        for i in range(n_points):\n",
        "            xs_comb = torch.cat((xs[:, :i, :], xs_p[:, i:, :]), dim=1)\n",
        "            ys = task.evaluate(xs_comb)\n",
        "\n",
        "            pred = model(xs_comb.to(device), ys.to(device), inds=[i]).detach()\n",
        "            metrics[:, i] = task.get_metric()(pred.cpu(), ys)[:, i]\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "# Functions for generating different kinds of train/test data\n",
        "\n",
        "\n",
        "def gen_standard(data_sampler, n_points, b_size):\n",
        "    xs = data_sampler.sample_xs(n_points, b_size)\n",
        "\n",
        "    return xs, None\n",
        "\n",
        "\n",
        "def gen_opposite_quadrants(data_sampler, n_points, b_size):\n",
        "    xs = data_sampler.sample_xs(n_points, b_size)\n",
        "    pattern = torch.randn([b_size, 1, xs.shape[2]]).sign()\n",
        "\n",
        "    xs_train_pre = xs.abs() * pattern\n",
        "    xs_test_post = -xs_train_pre\n",
        "\n",
        "    return xs_train_pre, xs_test_post\n",
        "\n",
        "\n",
        "def gen_random_quadrants(data_sampler, n_points, b_size):\n",
        "    xs = data_sampler.sample_xs(n_points, b_size)\n",
        "    pattern = torch.randn([b_size, 1, xs.shape[2]]).sign()\n",
        "\n",
        "    xs_train_pre = xs.abs() * pattern\n",
        "    xs_test_post = xs\n",
        "\n",
        "    return xs_train_pre, xs_test_post\n",
        "\n",
        "\n",
        "def gen_orthogonal_train_test(data_sampler, n_points, b_size):\n",
        "    xs = data_sampler.sample_xs(n_points, b_size)\n",
        "    n_dim = xs.shape[2]\n",
        "    n_points = min(n_points, n_dim)\n",
        "    # raise ValueError(\"number of points should be at most the dimension.\")\n",
        "    xs_train_pre = xs\n",
        "    xs_test_post = torch.zeros(xs.shape)\n",
        "    for i in range(n_points):\n",
        "        xs_test_post_i = xs[:, i : i + 1, :]\n",
        "        xs_train_pre_i = xs[:, :i, :]\n",
        "        _, _, Vt = torch.linalg.svd(xs_train_pre_i, full_matrices=False)\n",
        "        xs_train_pre_i_projection = Vt.transpose(1, 2) @ Vt\n",
        "        xs_test_post_i_orthogonalized = (\n",
        "            xs_test_post_i - xs_test_post_i @ xs_train_pre_i_projection\n",
        "        )\n",
        "        xs_test_post_i_normalized = (\n",
        "            xs_test_post_i_orthogonalized\n",
        "            * xs_test_post_i.norm(dim=2).unsqueeze(2)\n",
        "            / xs_test_post_i_orthogonalized.norm(dim=2).unsqueeze(2)\n",
        "        )\n",
        "\n",
        "        xs_test_post[:, i : i + 1, :] = xs_test_post_i_normalized\n",
        "\n",
        "    return xs_train_pre, xs_test_post\n",
        "\n",
        "\n",
        "def gen_overlapping_train_test(data_sampler, n_points, b_size):\n",
        "    xs = data_sampler.sample_xs(n_points, b_size)\n",
        "    xs_train_pre = xs\n",
        "    xs_test_post = xs.clone()\n",
        "    b_size = xs.shape[0]\n",
        "    for i in range(1, n_points):\n",
        "        xs_train_pre_i = xs[:, :i, :]\n",
        "        perm = torch.stack([torch.randperm(i) for _ in range(b_size)]).unsqueeze(dim=1)\n",
        "        ind_mat = (perm == 0) + 0.0\n",
        "        xs_test_post[:, i : i + 1, :] = ind_mat @ xs_train_pre_i\n",
        "\n",
        "    return xs_train_pre, xs_test_post\n",
        "\n",
        "\n",
        "def aggregate_metrics(metrics, bootstrap_trials=1000):\n",
        "    \"\"\"\n",
        "    Takes as input a tensor of shape (num_eval, n_points) and returns a dict with\n",
        "    per-point mean, stddev, and bootstrap limits\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    results[\"mean\"] = metrics.mean(dim=0)\n",
        "    results[\"std\"] = metrics.std(dim=0, unbiased=True)\n",
        "    n = len(metrics)\n",
        "    bootstrap_indices = torch.randint(n, size=(bootstrap_trials, n))\n",
        "    bootstrap_means = metrics[bootstrap_indices].mean(dim=1).sort(dim=0)[0]\n",
        "    results[\"bootstrap_low\"] = bootstrap_means[int(0.05 * bootstrap_trials), :]\n",
        "    results[\"bootstrap_high\"] = bootstrap_means[int(0.95 * bootstrap_trials), :]\n",
        "\n",
        "    return {k: v.tolist() for k, v in results.items()}\n",
        "\n",
        "\n",
        "def eval_model(\n",
        "    model,\n",
        "    task_name,\n",
        "    data_name,\n",
        "    n_dims,\n",
        "    n_points,\n",
        "    prompting_strategy,\n",
        "    num_eval_examples=1280,\n",
        "    batch_size=64,\n",
        "    data_sampler_kwargs={},\n",
        "    task_sampler_kwargs={},\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluate a model on a task with a variety of strategies.\n",
        "       Args:\n",
        "       - task: which base task we are evaluating on. E.g., \"linear_regression\"\n",
        "       - prompting_strategy: how to construct the prompt, e.g., \"random_quadrants\"\n",
        "       - num_eval_examples: total number of examples to evaluate on\n",
        "       - **sampler_kwargs: remaining arguments to pass directly to the sampler\n",
        "    \"\"\"\n",
        "\n",
        "    assert num_eval_examples % batch_size == 0\n",
        "    data_sampler = get_data_sampler(data_name, n_dims, **data_sampler_kwargs)\n",
        "    task_sampler = get_task_sampler(\n",
        "        task_name, n_dims, batch_size, **task_sampler_kwargs\n",
        "    )\n",
        "\n",
        "    all_metrics = []\n",
        "\n",
        "    generating_func = globals()[f\"gen_{prompting_strategy}\"]\n",
        "    for i in range(num_eval_examples // batch_size):\n",
        "        xs, xs_p = generating_func(data_sampler, n_points, batch_size)\n",
        "\n",
        "        metrics = eval_batch(model, task_sampler, xs, xs_p)\n",
        "        all_metrics.append(metrics)\n",
        "\n",
        "    metrics = torch.cat(all_metrics, dim=0)\n",
        "\n",
        "    return aggregate_metrics(metrics)\n",
        "\n",
        "\n",
        "def build_evals(conf):\n",
        "    n_dims = conf.model.n_dims\n",
        "    n_points = conf.training.curriculum.points.end\n",
        "    batch_size = conf.training.batch_size\n",
        "\n",
        "    task_name = conf.training.task\n",
        "    data_name = conf.training.data\n",
        "\n",
        "    base_kwargs = {\n",
        "        \"task_name\": task_name,\n",
        "        \"n_dims\": n_dims,\n",
        "        \"n_points\": n_points,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"data_name\": data_name,\n",
        "        \"prompting_strategy\": \"standard\",\n",
        "    }\n",
        "\n",
        "    evaluation_kwargs = {}\n",
        "\n",
        "    evaluation_kwargs[\"standard\"] = {\"prompting_strategy\": \"standard\"}\n",
        "    if task_name != \"linear_regression\":\n",
        "        if task_name in [\"relu_2nn_regression\"]:\n",
        "            evaluation_kwargs[\"linear_regression\"] = {\"task_name\": \"linear_regression\"}\n",
        "        for name, kwargs in evaluation_kwargs.items():\n",
        "            # allow kwargs to override base_kwargs values\n",
        "            evaluation_kwargs[name] = base_kwargs.copy()\n",
        "            evaluation_kwargs[name].update(kwargs)\n",
        "        return evaluation_kwargs\n",
        "\n",
        "    for strategy in [\n",
        "        \"random_quadrants\",\n",
        "        \"orthogonal_train_test\",\n",
        "        \"overlapping_train_test\",\n",
        "    ]:\n",
        "        evaluation_kwargs[strategy] = {\"prompting_strategy\": strategy}\n",
        "\n",
        "    for method in [\"half_subspace\", \"skewed\"]:\n",
        "        if \"subspace\" in method:\n",
        "            eigenvals = torch.zeros(n_dims)\n",
        "            eigenvals[: n_dims // 2] = 1\n",
        "        else:\n",
        "            eigenvals = 1 / (torch.arange(n_dims) + 1)\n",
        "\n",
        "        scale = sample_transformation(eigenvals, normalize=True)\n",
        "        evaluation_kwargs[f\"{method}\"] = {\n",
        "            \"data_sampler_kwargs\": {\"scale\": scale},\n",
        "        }\n",
        "\n",
        "    for dim in [\"x\", \"y\"]:\n",
        "        for scale in [0.333, 0.5, 2, 3]:\n",
        "            if dim == \"x\":\n",
        "                eigenvals = scale * torch.ones(n_dims)\n",
        "                t = sample_transformation(eigenvals)\n",
        "                scaling_args = {\"data_sampler_kwargs\": {\"scale\": t}}\n",
        "            else:\n",
        "                eigenvals = scale * torch.ones(n_dims)\n",
        "                scaling_args = {\"task_sampler_kwargs\": {\"scale\": scale}}\n",
        "\n",
        "            evaluation_kwargs[f\"scale-{dim}={scale}\"] = scaling_args\n",
        "\n",
        "    evaluation_kwargs[f\"noisyLR\"] = {\n",
        "        \"task_sampler_kwargs\": {\"renormalize_ys\": True, \"noise_std\": 1},\n",
        "        \"task_name\": \"noisy_linear_regression\",\n",
        "    }\n",
        "\n",
        "    for name, kwargs in evaluation_kwargs.items():\n",
        "        # allow kwargs to override base_kwargs values\n",
        "        evaluation_kwargs[name] = base_kwargs.copy()\n",
        "        evaluation_kwargs[name].update(kwargs)\n",
        "\n",
        "    return evaluation_kwargs\n",
        "\n",
        "\n",
        "def compute_evals(all_models, evaluation_kwargs, save_path=None, recompute=False):\n",
        "    try:\n",
        "        with open(save_path) as fp:\n",
        "            all_metrics = json.load(fp)\n",
        "    except Exception:\n",
        "        all_metrics = {}\n",
        "\n",
        "    for eval_name, kwargs in tqdm(evaluation_kwargs.items()):\n",
        "        metrics = {}\n",
        "        if eval_name in all_metrics and not recompute:\n",
        "            metrics = all_metrics[eval_name]\n",
        "        for model in all_models:\n",
        "            if model.name in metrics and not recompute:\n",
        "                continue\n",
        "\n",
        "            metrics[model.name] = eval_model(model, **kwargs)\n",
        "        all_metrics[eval_name] = metrics\n",
        "\n",
        "    if save_path is not None:\n",
        "        with open(save_path, \"w\") as fp:\n",
        "            json.dump(all_metrics, fp, indent=2)\n",
        "\n",
        "    return all_metrics\n",
        "\n",
        "\n",
        "def get_run_metrics(\n",
        "    run_path, step=-1, cache=True, skip_model_load=False, skip_baselines=False\n",
        "):\n",
        "    if skip_model_load:\n",
        "        _, conf = get_model_from_run(run_path, only_conf=True)\n",
        "        all_models = []\n",
        "    else:\n",
        "        model, conf = get_model_from_run(run_path, step)\n",
        "        model = model.cuda().eval()\n",
        "        all_models = [model]\n",
        "        if not skip_baselines:\n",
        "            all_models += get_relevant_baselines(conf.training.task)\n",
        "    evaluation_kwargs = build_evals(conf)\n",
        "\n",
        "    if not cache:\n",
        "        save_path = None\n",
        "    elif step == -1:\n",
        "        save_path = os.path.join(run_path, \"metrics.json\")\n",
        "    else:\n",
        "        save_path = os.path.join(run_path, f\"metrics_{step}.json\")\n",
        "\n",
        "    recompute = False\n",
        "    if save_path is not None and os.path.exists(save_path):\n",
        "        checkpoint_created = os.path.getmtime(run_path)\n",
        "        cache_created = os.path.getmtime(save_path)\n",
        "        if checkpoint_created > cache_created:\n",
        "            recompute = True\n",
        "\n",
        "    all_metrics = compute_evals(all_models, evaluation_kwargs, save_path, recompute)\n",
        "    return all_metrics\n",
        "\n",
        "\n",
        "\n",
        "def conf_to_model_name(conf):\n",
        "    if conf.model.family == \"gpt2\":\n",
        "        return {\n",
        "            (3, 2): \"Transformer-xs\",\n",
        "            (6, 4): \"Transformer-small\",\n",
        "            (12, 8): \"Transformer\",\n",
        "        }[(conf.model.n_layer, conf.model.n_head)]\n",
        "    else:\n",
        "        return conf.wandb.name\n",
        "\n",
        "\n",
        "def baseline_names(name):\n",
        "    if \"OLS\" in name:\n",
        "        return \"Least Squares\"\n",
        "    if name == \"averaging\":\n",
        "        return \"Averaging\"\n",
        "    if \"NN\" in name:\n",
        "        k = name.split(\"_\")[1].split(\"=\")[1]\n",
        "        return f\"{k}-Nearest Neighbors\"\n",
        "    if \"lasso\" in name:\n",
        "        alpha = name.split(\"_\")[1].split(\"=\")[1]\n",
        "        return f\"Lasso (alpha={alpha})\"\n",
        "    if \"gd\" in name:\n",
        "        return \"2-layer NN, GD\"\n",
        "    if \"decision_tree\" in name:\n",
        "        return \"Greedy Tree Learning\"\n",
        "    if \"xgboost\" in name:\n",
        "        return \"XGBoost\"\n",
        "    return name\n",
        "\n",
        "\n",
        "def read_run_dir(run_dir):\n",
        "    all_runs = {}\n",
        "    for task in os.listdir(run_dir):\n",
        "        task_dir = os.path.join(run_dir, task)\n",
        "        for run_id in os.listdir(task_dir):\n",
        "            run_path = os.path.join(task_dir, run_id)\n",
        "            _, conf = get_model_from_run(run_path, only_conf=True)\n",
        "            params = {}\n",
        "            params[\"run_id\"] = run_id\n",
        "            params[\"task\"] = task\n",
        "            params[\"model\"] = conf_to_model_name(conf)\n",
        "            params[\"kwargs\"] = \"_\".join(\n",
        "                f\"{k}={v}\" for k, v in conf.training.task_kwargs.items()\n",
        "            )\n",
        "            num_tasks = (\n",
        "                conf.training.num_tasks if \"num_tasks\" in conf.training else None\n",
        "            )\n",
        "            params[\"num_tasks\"] = num_tasks if num_tasks is not None else -1\n",
        "            num_examples = (\n",
        "                conf.training.num_training_examples\n",
        "                if \"num_training_examples\" in conf.training\n",
        "                else None\n",
        "            )\n",
        "            params[\"num_examples\"] = num_examples if num_examples is not None else -1\n",
        "            params[\"n_dims\"] = conf.model.n_dims\n",
        "            params[\"n_layer\"] = conf.model.n_layer\n",
        "            params[\"n_head\"] = conf.model.n_head\n",
        "            params[\"run_name\"] = conf.wandb.name\n",
        "\n",
        "            for k, v in params.items():\n",
        "                if k not in all_runs:\n",
        "                    all_runs[k] = []\n",
        "                all_runs[k].append(v)\n",
        "\n",
        "    df = pd.DataFrame(all_runs).sort_values(\"run_name\")\n",
        "    assert len(df) == len(df.run_name.unique())\n",
        "    return df"
      ],
      "metadata": {
        "id": "QhjsW5C6E_GC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train\n"
      ],
      "metadata": {
        "id": "ByhU5udsu8o0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from random import randint\n",
        "import uuid\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "import wandb"
      ],
      "metadata": {
        "id": "aScQG1VoDMMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "def train_step(model, xs, ys, optimizer, loss_func):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(xs, ys)\n",
        "    loss = loss_func(output, ys)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.detach().item(), output.detach()\n",
        "\n",
        "\n",
        "def sample_seeds(total_seeds, count):\n",
        "    seeds = set()\n",
        "    while len(seeds) < count:\n",
        "        seeds.add(randint(0, total_seeds - 1))\n",
        "    return seeds\n",
        "\n",
        "\n",
        "def train(model, args):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.training.learning_rate)\n",
        "    curriculum = Curriculum(args.training.curriculum)\n",
        "\n",
        "    starting_step = 0\n",
        "    state_path = os.path.join(args.out_dir, \"state.pt\")\n",
        "    if os.path.exists(state_path):\n",
        "        state = torch.load(state_path)\n",
        "        model.load_state_dict(state[\"model_state_dict\"])\n",
        "        optimizer.load_state_dict(state[\"optimizer_state_dict\"])\n",
        "        starting_step = state[\"train_step\"]\n",
        "        for i in range(state[\"train_step\"] + 1):\n",
        "            curriculum.update()\n",
        "\n",
        "    n_dims = model.n_dims\n",
        "    bsize = args.training.batch_size\n",
        "    data_sampler = get_data_sampler(args.training.data, n_dims=n_dims)\n",
        "    task_sampler = get_task_sampler(\n",
        "        args.training.task,\n",
        "        n_dims,\n",
        "        bsize,\n",
        "        num_tasks=args.training.num_tasks,\n",
        "        **args.training.task_kwargs,\n",
        "    )\n",
        "    pbar = tqdm(range(starting_step, args.training.train_steps))\n",
        "\n",
        "    num_training_examples = args.training.num_training_examples\n",
        "\n",
        "    for i in pbar:\n",
        "        data_sampler_args = {}\n",
        "        task_sampler_args = {}\n",
        "\n",
        "        if \"sparse\" in args.training.task:\n",
        "            task_sampler_args[\"valid_coords\"] = curriculum.n_dims_truncated\n",
        "        if num_training_examples is not None:\n",
        "            assert num_training_examples >= bsize\n",
        "            seeds = sample_seeds(num_training_examples, bsize)\n",
        "            data_sampler_args[\"seeds\"] = seeds\n",
        "            task_sampler_args[\"seeds\"] = [s + 1 for s in seeds]\n",
        "\n",
        "        xs = data_sampler.sample_xs(\n",
        "            curriculum.n_points,\n",
        "            bsize,\n",
        "            curriculum.n_dims_truncated,\n",
        "            **data_sampler_args,\n",
        "        )\n",
        "        task = task_sampler(**task_sampler_args)\n",
        "        ys = task.evaluate(xs)\n",
        "\n",
        "        loss_func = task.get_training_metric()\n",
        "\n",
        "        loss, output = train_step(model, xs.cuda(), ys.cuda(), optimizer, loss_func)\n",
        "\n",
        "        point_wise_tags = list(range(curriculum.n_points))\n",
        "        point_wise_loss_func = task.get_metric()\n",
        "        point_wise_loss = point_wise_loss_func(output, ys.cuda()).mean(dim=0)\n",
        "\n",
        "        baseline_loss = (\n",
        "            sum(\n",
        "                max(curriculum.n_dims_truncated - ii, 0)\n",
        "                for ii in range(curriculum.n_points)\n",
        "            )\n",
        "            / curriculum.n_points\n",
        "        )\n",
        "\n",
        "        if i % args.wandb.log_every_steps == 0 and not args.test_run and args.using_wandb:\n",
        "            wandb.log(\n",
        "                {\n",
        "                    \"overall_loss\": loss,\n",
        "                    \"excess_loss\": loss / baseline_loss,\n",
        "                    \"pointwise/loss\": dict(\n",
        "                        zip(point_wise_tags, point_wise_loss.cpu().numpy())\n",
        "                    ),\n",
        "                    \"n_points\": curriculum.n_points,\n",
        "                    \"n_dims\": curriculum.n_dims_truncated,\n",
        "                },\n",
        "                step=i,\n",
        "            )\n",
        "\n",
        "        curriculum.update()\n",
        "\n",
        "        pbar.set_description(f\"loss {loss}\")\n",
        "        if i % args.training.save_every_steps == 0 and not args.test_run:\n",
        "            training_state = {\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"train_step\": i,\n",
        "            }\n",
        "            torch.save(training_state, state_path)\n",
        "\n",
        "        if (\n",
        "            args.training.keep_every_steps > 0\n",
        "            and i % args.training.keep_every_steps == 0\n",
        "            and not args.test_run\n",
        "            and i > 0\n",
        "        ):\n",
        "            torch.save(model.state_dict(), os.path.join(args.out_dir, f\"model_{i}.pt\"))"
      ],
      "metadata": {
        "id": "8M4Uj024u9PG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if args.using_wandb:\n",
        "    wandb.init(\n",
        "        dir=args.out_dir,\n",
        "        project=args.wandb.project,\n",
        "        entity=args.wandb.entity,\n",
        "        config=args.__dict__,\n",
        "        notes=args.wandb.notes,\n",
        "        name=args.wandb.name,\n",
        "        resume=True,\n",
        "    )\n",
        "\n",
        "run_id = args.training.resume_id\n",
        "if run_id is None:\n",
        "    run_id = str(uuid.uuid4())\n",
        "\n",
        "out_dir = os.path.join(args.out_dir, run_id)\n",
        "if not os.path.exists(out_dir):\n",
        "    os.makedirs(out_dir)\n",
        "args.out_dir = out_dir\n",
        "\n",
        "model = build_model(args.model)\n",
        "model.cuda()\n",
        "model.train()\n",
        "\n",
        "train(model, args)\n",
        "\n",
        "_ = get_run_metrics(args.out_dir)  # precompute metrics for eval\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQrcM2wIwgZg",
        "outputId": "2accc592-d952-451d-a212-552f9efeb12a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 5.779212951660156:   8%|         | 41423/500001 [34:58<7:49:58, 16.26it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluate on new data\n",
        "\n"
      ],
      "metadata": {
        "id": "8ANEUBJNpYOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot Utils\n"
      ],
      "metadata": {
        "id": "SSby9hviLhEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "7AcnzjkGLjaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_theme(\"notebook\", \"darkgrid\")\n",
        "palette = sns.color_palette(\"colorblind\")\n",
        "\n",
        "\n",
        "relevant_model_names = {\n",
        "    \"linear_regression\": [\n",
        "        \"Transformer\",\n",
        "        \"Least Squares\",\n",
        "        \"3-Nearest Neighbors\",\n",
        "        \"Averaging\",\n",
        "    ],\n",
        "    \"sparse_linear_regression\": [\n",
        "        \"Transformer\",\n",
        "        \"Least Squares\",\n",
        "        \"3-Nearest Neighbors\",\n",
        "        \"Averaging\",\n",
        "        \"Lasso (alpha=0.01)\",\n",
        "    ],\n",
        "    \"decision_tree\": [\n",
        "        \"Transformer\",\n",
        "        \"3-Nearest Neighbors\",\n",
        "        \"2-layer NN, GD\",\n",
        "        \"Greedy Tree Learning\",\n",
        "        \"XGBoost\",\n",
        "    ],\n",
        "    \"relu_2nn_regression\": [\n",
        "        \"Transformer\",\n",
        "        \"Least Squares\",\n",
        "        \"3-Nearest Neighbors\",\n",
        "        \"2-layer NN, GD\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "\n",
        "def basic_plot(metrics, models=None, trivial=1.0):\n",
        "    fig, ax = plt.subplots(1, 1)\n",
        "\n",
        "    if models is not None:\n",
        "        metrics = {k: metrics[k] for k in models}\n",
        "\n",
        "    color = 0\n",
        "    ax.axhline(trivial, ls=\"--\", color=\"gray\")\n",
        "    for name, vs in metrics.items():\n",
        "        ax.plot(vs[\"mean\"], \"-\", label=name, color=palette[color % 10], lw=2)\n",
        "        low = vs[\"bootstrap_low\"]\n",
        "        high = vs[\"bootstrap_high\"]\n",
        "        ax.fill_between(range(len(low)), low, high, alpha=0.3)\n",
        "        color += 1\n",
        "    ax.set_xlabel(\"in-context examples\")\n",
        "    ax.set_ylabel(\"squared error\")\n",
        "    ax.set_xlim(-1, len(low) + 0.1)\n",
        "    ax.set_ylim(-0.1, 1.25)\n",
        "\n",
        "    legend = ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
        "    fig.set_size_inches(4, 3)\n",
        "    for line in legend.get_lines():\n",
        "        line.set_linewidth(3)\n",
        "\n",
        "    return fig, ax\n",
        "\n",
        "\n",
        "def collect_results(run_path, task):\n",
        "    all_metrics = {}\n",
        "\n",
        "    _, conf = get_model_from_run(run_path, only_conf=True)\n",
        "\n",
        "    print(run_path)\n",
        "    metrics = get_run_metrics(run_path, skip_model_load=True)\n",
        "\n",
        "    for eval_name, results in sorted(metrics.items()):\n",
        "        processed_results = {}\n",
        "        for model_name, m in results.items():\n",
        "            if \"gpt2\" in model_name in model_name:\n",
        "                model_name = \"Transformer\"\n",
        "            else:\n",
        "                model_name = baseline_names(model_name)\n",
        "            m_processed = {}\n",
        "            n_dims = conf.model.n_dims\n",
        "\n",
        "            xlim = 2 * n_dims + 1\n",
        "            if task in [\"relu_2nn_regression\", \"decision_tree\"]:\n",
        "                xlim = 200\n",
        "\n",
        "            normalization = n_dims\n",
        "            if task == \"sparse_linear_regression\":\n",
        "                normalization = int(args.training.task_kwargs.split(\"=\")[-1])\n",
        "            if task == \"decision_tree\":\n",
        "                normalization = 1\n",
        "\n",
        "            for k, v in m.items():\n",
        "                v = v[:xlim]\n",
        "                v = [vv / normalization for vv in v]\n",
        "                m_processed[k] = v\n",
        "            processed_results[model_name] = m_processed\n",
        "        if eval_name not in all_metrics:\n",
        "            all_metrics[eval_name] = {}\n",
        "        all_metrics[eval_name].update(processed_results)\n",
        "    return all_metrics"
      ],
      "metadata": {
        "id": "iTmye-K5LifU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate Model"
      ],
      "metadata": {
        "id": "9eVIiFOzLog4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "import re\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "sns.set_theme('notebook', 'darkgrid')\n",
        "palette = sns.color_palette('colorblind')"
      ],
      "metadata": {
        "id": "pA_FGE25pY0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = args.training.task\n",
        "\n",
        "run_path = args.out_dir\n",
        "recompute_metrics = False\n",
        "\n",
        "if recompute_metrics:\n",
        "    get_run_metrics(run_path)  # these are normally precomputed at the end of training"
      ],
      "metadata": {
        "id": "gmKsIVZjpeN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot precomputed metrics"
      ],
      "metadata": {
        "id": "6OBV59Dppg2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = collect_results(run_path, task)\n",
        "_, conf = get_model_from_run(run_path, only_conf=True)\n",
        "n_dims = conf.model.n_dims\n",
        "\n",
        "models = relevant_model_names[task]\n",
        "basic_plot(metrics[\"standard\"], models=models)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LRJ2R4lhpfcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot any OOD metrics\n",
        "for name, metric in metrics.items():\n",
        "    if name == \"standard\": continue\n",
        "\n",
        "    if \"scale\" in name:\n",
        "        scale = float(name.split(\"=\")[-1])**2\n",
        "    else:\n",
        "        scale = 1.0\n",
        "\n",
        "    trivial = 1.0 if \"noisy\" not in name else (1+1/n_dims)\n",
        "    fig, ax = basic_plot(metric, models=models, trivial=trivial * scale)\n",
        "    ax.set_title(name)\n",
        "\n",
        "    if \"ortho\" in name:\n",
        "        ax.set_xlim(-1, n_dims - 1)\n",
        "    ax.set_ylim(-.1 * scale, 1.5 * scale)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "PhkgGW1WplF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompting\n"
      ],
      "metadata": {
        "id": "wx02SWrspoDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, conf = get_model_from_run(run_path)\n",
        "\n",
        "n_dims = conf.model.n_dims\n",
        "batch_size = conf.training.batch_size\n",
        "\n",
        "data_sampler = get_data_sampler(conf.training.data, n_dims)\n",
        "task_sampler = get_task_sampler(\n",
        "    conf.training.task,\n",
        "    n_dims,\n",
        "    batch_size,\n",
        "    **conf.training.task_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "a2eawlGRpqTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = task_sampler()\n",
        "xs = data_sampler.sample_xs(b_size=batch_size, n_points=conf.training.curriculum.points.end)\n",
        "ys = task.evaluate(xs)"
      ],
      "metadata": {
        "id": "6q_tsQfPpsVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    pred = model(xs, ys)"
      ],
      "metadata": {
        "id": "lHXcz73ipvAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = task.get_metric()\n",
        "loss = metric(pred, ys).numpy()\n",
        "\n",
        "sparsity = conf.training.task_kwargs.sparsity if \"sparsity\" in conf.training.task_kwargs else None\n",
        "baseline = {\n",
        "    \"linear_regression\": n_dims,\n",
        "    \"sparse_linear_regression\": sparsity,\n",
        "    \"relu_2nn_regression\": n_dims,\n",
        "    \"decision_tree\": 1,\n",
        "}[conf.training.task]\n",
        "\n",
        "plt.plot(loss.mean(axis=0), lw=2, label=\"Transformer\")\n",
        "plt.axhline(baseline, ls=\"--\", color=\"gray\", label=\"zero estimator\")\n",
        "plt.xlabel(\"# in-context examples\")\n",
        "plt.ylabel(\"squared error\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6hundlcKpwDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FJ7gN6v4Ua7l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}